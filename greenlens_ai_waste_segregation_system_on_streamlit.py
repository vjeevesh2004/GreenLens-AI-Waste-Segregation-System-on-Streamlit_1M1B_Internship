# -*- coding: utf-8 -*-
"""GreenLens: AI Waste Segregation System on Streamlit

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AXsodPnjOMW8_WVEJuca5kcsOoqKquU6

# Problem Statement <a id="problem-statement"></a>:
The goal of this project is to develop an automated waste classification system using state-of-the-art deep learning techniques. A fine-tuned '**DenseNet169**' model is employed to classify waste images into predefined categories, trained on a dataset of 15,000 images representing different types of waste. The project involves image preprocessing and advanced augmentation techniques such as CutMix and MixUp from KerasCV to enhance model generalization and robustness. By leveraging the hierarchical feature extraction capability of **DenseNet169** along with these augmentation strategies, the system aims to achieve high accuracy and reliable performance in real-world scenarios. The successful implementation of this model will minimize human error in waste sorting, streamline recycling processes, and contribute significantly to promoting environmental sustainability.
"""

!pip install kaggle
from google.colab import files

# this is imp to import files from kaggle to collab
import kagglehub
alistairking_recyclable_and_household_waste_classification_path = kagglehub.dataset_download('alistairking/recyclable-and-household-waste-classification')
keras_efficientnetv2_keras_efficientnetv2_b2_imagenet_2_path = kagglehub.model_download('keras/efficientnetv2/Keras/efficientnetv2_b2_imagenet/2')

print('Data source import complete.')

!nvidia-smi

"""---

# Imports <a id="imports"></a>:
"""

!pip install keras_cv

import os
import shutil
import keras

import random
import keras_cv
import warnings
import numpy as np
import pandas as pd
import seaborn as sns
import tensorflow as tf
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
from sklearn.metrics import accuracy_score
from keras.utils import image_dataset_from_directory

# Setting up Keras Backend
os.environ["KERAS_BACKEND"] = "jax"

warnings.filterwarnings("ignore")

# Reproducability
tf.keras.utils.set_random_seed(73)

"""---

# Exploring the Working Directory <a id="exploring-the-working-directory"></a>:

The current structure of the `/kaggle/input/` directory is as follows:

- /kaggle/input/recyclable-and-household-waste-classification/images/images/

  - aerosol_cans/
    - default/
      - image1.png
      - image2.png
      - ...
    - real_world/
      - image1.png
      - image2.png
      - ...
      
      
  - aluminium_food_cans/
    - default/
      - image1.png
      - image2.png
      - ...
    - real_world/
      - image1.png
      - image2.png
      - ...
      
      
  - aluminum_soda_cans/
    - default/
      - image1.png
      - image2.png
      - ...
    - real_world/
      - image1.png
      - image2.png
      - ...
     
     
  - ...
"""

# Creating a list of directories of all image categories in /kaggle/input/

waste_type_dir = []

for (root, dirs, files) in os.walk('/kaggle/input'):
    if root.split('/')[-3:-1] == ['images', 'images']:
        waste_type_dir.append(root)

waste_type_dir.sort()
waste_type_dir[:5]

"""# Creating Train, Test, and Validation Directories <a id="creating-train-test-and-validation-directories"></a>:"""

# Defining the paths for training, validation, test directories
train_dir = '/kaggle/working/training-images/'
valid_dir = '/kaggle/working/validation-images/'
test_dir = '/kaggle/working/test-images/'

# Creating the directories for training and test images and also creating sub-directories for each waste category
for x in waste_type_dir:
    os.makedirs(train_dir + x.split('/')[-1], exist_ok=True)
    os.makedirs(valid_dir + x.split('/')[-1], exist_ok=True)
    os.makedirs(test_dir + x.split('/')[-1], exist_ok=True)

"""---

# EDA <a id="EDA"></a>:
"""

# Creating a dataframe of waste categories and the corressponding number of images in each category

num_images_dict = {'waste_category': [x.split('/')[-1] for x in waste_type_dir],
             'num_images': [(len(os.listdir(x + '/default')) + len(os.listdir(x + '/real_world'))) for x in waste_type_dir],
             'num_default_images': [len(os.listdir(x + '/default')) for x in waste_type_dir],
             'num_real_world_images': [len(os.listdir(x + '/real_world')) for x in waste_type_dir]}

num_images_df = pd.DataFrame.from_dict(num_images_dict)

plt.figure(figsize=(12, 6), dpi=150)
plt.title("Number of default images in each waste category")
plt.xticks(rotation=90)
ax = sns.barplot(num_images_df, x='waste_category', y='num_default_images')
ax.bar_label(ax.containers[0], fontsize=10);

plt.figure(figsize=(12, 6), dpi=150)
plt.title("Number of real world images in each waste category")
plt.xticks(rotation=90)
ax = sns.barplot(num_images_df, x='waste_category', y='num_real_world_images')
ax.bar_label(ax.containers[0], fontsize=10);

"""* As you can see that, we have a balanced dataset because all the categories have the same number of images."""

# Function for plotting 2 random images from a folder

def plot_random_images_from_directory(directory_path):

    all_files = os.listdir(directory_path)

    if len(all_files) < 4:
        print("Not enough images in the directory to display.")
        return

    selected_images = random.sample(all_files, 2)
    fig, axes = plt.subplots(1, 2, figsize=(6, 3), dpi=100)

    for i, ax in enumerate(axes.flat):
        image_path = os.path.join(directory_path, selected_images[i])
        img = mpimg.imread(image_path)
        ax.imshow(img)
        ax.axis('off')

    class_name = os.path.basename(directory_path)
    fig.suptitle(directory_path.split('/')[-2] + ': ' + class_name, fontsize=16)
    plt.tight_layout()
    plt.show()

# Visualizing 2 random default and real_world images of every waste category

for directory in waste_type_dir:
    plot_random_images_from_directory(directory + '/default')
    plot_random_images_from_directory(directory + '/real_world')

"""---

# Copying the Images <a id="copying-the-images"></a>:

The images in `/kaggle/input/` are not seggregated into train, test and validation directories. So, we will first copy all the 15,000 images to `/kaggle/working/training-images` directory. From there, we will move 50 images of each waste category to the `/kaggle/working/validation-images` and `/kaggle/working/test-images`. The final structure of `/kaggle/working/` would look as follows:


- /kaggle/working/


    - training-images/
        - aerosol_cans/
            - image1.png
            - image2.png
            - image3.png
            - ...
        - aluminium_food_cans/
            - image1.png
            - image2.png
            - image3.png
            - ...
        - aluminum_soda_cans/
            - image1.png
            - image2.png
            - image3.png
            - ...
        - ...


    - validation-images/
        - aerosol_cans/
            - image1.png
            - image2.png
            - image3.png
            - ...
        - aluminium_food_cans/
            - image1.png
            - image2.png
            - image3.png
            - ...
        - aluminium_soda_cans/
            - image1.png
            - image2.png
            - image3.png
            - ...
        - ...


    - test-images/
        - aerosol_cans/
            - image1.png
            - image2.png
            - image3.png
            - ...
        - aluminium_food_cans/
            - image1.png
            - image2.png
            - image3.png
            - ...
        - aluminium_soda_cans/
            - image1.png
            - image2.png
            - image3.png
            - ...
        - ...
"""

# Creating a list of all image paths in /kaggle/input/

file_paths = []

for directory in waste_type_dir:
    for root, dirs, files in os.walk(directory):
        if root.split('/')[-1] == 'default' or root.split('/')[-1] == 'real_world':
            paths = os.listdir(root)
            for path in paths:
                file_paths.append(os.path.join(root, path))

file_paths.sort()

if len(file_paths) == len(set(file_paths)):
    print("There are no duplicate file paths in the list.")
else:
    print(f"There are {len(file_paths)-len(set(file_paths))} duplicate file paths in the list.")

# Copyling all the images to the sub-directories of /kaggle/working/training-images/ according to their waste category

num=0

for src in file_paths:
    fn = src.split('/')[-1]
    dst = train_dir + src.split('/')[-3] + '/'
    if fn not in os.listdir(dst):
        shutil.copy(src, dst)
        num+=1
    elif fn in os.listdir(dst):
        new_fn = 'Image_' + str(250 + int(fn.split('_')[-1].split('.')[0])) + '.png'
        shutil.copy(src, dst + new_fn)
        num+=1

print(f"Fininshed copying {num} images.")

# Moving 50 images of each waste category from the training-images directory to the validation-images and test-images directory

for root, dirs, files in os.walk('/kaggle/working/training-images'):
    if root.split('/')[-1] != 'training-images':
        for i in range(50):
            src_path_test = os.path.join(root, random.choice(os.listdir(root)))
            dst_path_test = '/kaggle/working/test-images/' + root.split('/')[-1]
            shutil.move(src_path_test, dst_path_test)
            src_path_valid = os.path.join(root, random.choice(os.listdir(root)))
            dst_path_valid = '/kaggle/working/validation-images/' + root.split('/')[-1]
            shutil.move(src_path_valid, dst_path_valid)

print("50 images of each waste category were moved to the validation and test directories.")

"""* **NOTE:** We won't carry out any image preprocessing here because all the images in the dataset are already 256 by 256 pixels in size and are in png format.

---

# Building Tensorflow Image Datasets <a id="building-tensorflow-image-datasets"></a>:
"""

def build_img_dataset(directory, BATCH_SIZE=32, SEED=101, IMG_SIZE=(224, 224), class_names=False, shuffle=True):
    dataset = tf.keras.preprocessing.image_dataset_from_directory(
        directory=directory,
        batch_size=BATCH_SIZE,
        label_mode='categorical',
        image_size=IMG_SIZE,
        interpolation='bilinear',
        shuffle=shuffle,
        seed=SEED
    )

    class_names = dataset.class_names

    dataset = dataset.cache().prefetch(tf.data.AUTOTUNE)

    return (dataset, class_names if class_names else None)

train_data = build_img_dataset(train_dir)[0]
valid_data = build_img_dataset(valid_dir)[0]
test_data, test_classes = build_img_dataset(test_dir, class_names=True) # will need test_classes in visualizing predictions

"""---

```
# This is formatted as code
```

# Image Augmentations <a id="image-augmentations"></a>:

Data augmentation is a technique to make your model robust to changes in input data such as lighting, cropping, and orientation. One caveat to be aware of with image data augmentation is that you must be careful to not shift your augmented data distribution too far from the original data distribution. The goal is to prevent overfitting and increase generalization, but samples that lie completely out of the data distribution simply add noise to the training process.


[[source]](https://keras.io/guides/keras_cv/classification_with_keras_cv/)
"""

def pack_to_dict(image, label):
    return {"images": image, "labels": label}

train_data = train_data.map(pack_to_dict, num_parallel_calls=tf.data.AUTOTUNE)

batch = next(iter(train_data.take(1)))
image_batch = batch["images"]
label_batch = batch["labels"]

def visualize_images(rows=2, cols=2, augmentation=None, images=image_batch):
    keras_cv.visualization.plot_image_gallery(
        augmentation(images) if augmentation else images,
        rows = rows,
        cols = cols,
        value_range = (0, 255),
        show = True,
    )

# Images before applying augmentation
visualize_images()

"""### RandomFlip Augmentation:"""

random_flip = keras_cv.layers.RandomFlip(
    mode = "horizontal_and_vertical"
)

augmenters = [random_flip]
visualize_images(augmentation = random_flip)

"""### RandomCropAndResize Augmentation:

The next augmentation we'll use is RandomCropAndResize. This operation selects a random subset of the image, then resizes it to the provided target size. By using this augmentation, we force our classifier to become spatially invariant. Additionally, this layer accepts an ```aspect_ratio_factor``` which can be used to distort the aspect ratio of the image. While this can improve model performance, it should be used with caution. It is very easy for an aspect ratio distortion to shift a sample too far from the original training set's data distribution.


[[source]](https://keras.io/guides/keras_cv/classification_with_keras_cv/)
"""

random_crop_and_resize = keras_cv.layers.RandomCropAndResize(
    target_size = (224, 224),
    crop_area_factor = (0.8, 1.0),
    aspect_ratio_factor = (0.9, 1.1),
)

augmenters += [random_crop_and_resize]
visualize_images(augmentation = random_crop_and_resize)

"""### RandomCutOut Augmentation:

This augmentation technique randomly cuts out rectangles from images and fill them.
"""

random_cutout = keras_cv.layers.RandomCutout(
    height_factor = (0.5, 0.5),
    width_factor = (0.5, 0.5),
    fill_mode = 'gaussian_noise'
)

augmenters += [random_cutout]
visualize_images(augmentation = random_cutout)

"""### RandAugmentation Augmentation:

```RandAugment``` is actually a set of 10 different augmentations: ```AutoContrast```, ```Equalize```, ```Solarize```, ```RandomColorJitter```, ```RandomContrast```, ```RandomBrightness```, ```ShearX```, ```ShearY```, ```TranslateX``` and ```TranslateY```. At inference time, ```num_augmentations``` augmenters are sampled for each image, and random magnitude factors are sampled for each. These augmentations are then applied sequentially.


[[source]](https://keras.io/guides/keras_cv/classification_with_keras_cv/)
"""

rand_augment = keras_cv.layers.RandAugment(
    value_range = (0, 255),
    augmentations_per_image = 5,
    magnitude = 0,
    magnitude_stddev = 1,
    rate = 10/11,
)

augmenters += [rand_augment]
visualize_images(augmentation = rand_augment)

"""### CutMix Augmentation:

While the random cutouts of the ```RandomCutOut``` layer tackles the problem reasonably well, it can cause the classifier to develop responses to borders between features and the pixel areas caused by the cutout.

```CutMix``` solves the same issue by using a more complex (and more effective) technique. Instead of replacing the cut-out areas with random noise, ```CutMix``` replaces these regions with regions of other images sampled from within your training set! Following this replacement, the image's classification label is updated to be a blend of the original and mixed image's class label.

**NOTE:** ```CutMix``` needs to modify both images and labels


[[source]](https://keras.io/guides/keras_cv/classification_with_keras_cv/)
"""

cut_mix = keras_cv.layers.CutMix()
inputs = {"images": image_batch, "labels": label_batch}
visualize_images(images = cut_mix(inputs)['images'])

"""### MixUp Augmentation:

```MixUp()``` works by sampling two images from a batch, then proceeding to literally blend together their pixel intensities as well as their classification labels.

**NOTE:** ```MixUp``` needs to modify both images and labels


[[source]](https://keras.io/guides/keras_cv/classification_with_keras_cv/)
"""

mix_up = keras_cv.layers.MixUp(0.5)
inputs = {"images": image_batch, "labels": label_batch}
visualize_images(images = mix_up(inputs)['images'])

"""**NOTE:** Instead of applying ```CutMix()``` and ```MixUp()``` to every image, we will apply either of them to each batch. We will do this using ```keras_cv.layers.RandomChoice()```."""

cut_mix_or_mix_up = keras_cv.layers.RandomChoice(
    [cut_mix, mix_up],
    batchwise = True
)
augmenters += [cut_mix_or_mix_up]

"""### Creating and applying image augmentation function:"""

def create_augmenter_fn(augmenters):
    def augmenter_fn(inputs):
        for augmenter in augmenters:
            inputs = augmenter(inputs)
        return inputs
    return augmenter_fn


augmenter_fn = create_augmenter_fn(augmenters)
train_data = train_data.map(augmenter_fn, num_parallel_calls=tf.data.AUTOTUNE)

"""### Visualizing the augmented images:"""

batch = next(iter(train_data.take(1)))
image_batch = batch["images"]
label_batch = batch["labels"]

visualize_images(images = image_batch)

def unpackage_dict(inputs):
    return inputs["images"], inputs["labels"]

train_data = train_data.map(unpackage_dict, num_parallel_calls=tf.data.AUTOTUNE)

"""---

# Model Building <a id="Model Building"></a>:
"""

from tensorflow.keras.applications import DenseNet169
from tensorflow.keras.applications.densenet import preprocess_input
from tensorflow.keras import layers, models

""" Load DenseNet169 backbone"""

base_model = DenseNet169(
    weights='imagenet',       # load pretrained ImageNet weights
    include_top=False,        # exclude final classification layer
    input_shape=(224, 224, 3)
)

# Freeze the backbone initially
base_model.trainable = False

"""Build classification head"""

inputs = layers.Input(shape=(224,224,3))
x = preprocess_input(inputs)   # <- important!
x = base_model(x, training=False)
x = layers.GlobalAveragePooling2D()(x)
outputs = layers.Dense(len(test_classes), activation="softmax")(x)

model = models.Model(inputs, outputs)

model.compile(
    optimizer=tf.keras.optimizers.Adam(1e-4),
    loss="categorical_crossentropy",
    metrics=["accuracy"]
)

model.summary()

"""---

# Model Training <a id="model-training"></a>:
"""

earlystopping = keras.callbacks.EarlyStopping(
    monitor = 'val_accuracy',
    patience = 2,
    mode = 'max',
    min_delta = 0.005,
    verbose = 1,
    restore_best_weights = True
)

# history = model.fit(
#     train_data,
#     epochs = 20,
#     validation_data = valid_data,
#     callbacks = [earlystopping],
# )
history = model.fit(
    train_data,
    validation_data=valid_data,
    epochs=5,
    callbacks=[earlystopping]
)

"""“The model has overfitted multiple times, causing the code to crash.

---
"""